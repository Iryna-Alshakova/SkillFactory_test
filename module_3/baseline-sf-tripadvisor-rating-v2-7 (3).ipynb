{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://www.pata.org/wp-content/uploads/2014/09/TripAdvisor_Logo-300x119.png)\n# Predict TripAdvisor Rating\n## В этом соревновании нам предстоит предсказать рейтинг ресторана в TripAdvisor\n**По ходу задачи:**\n* Прокачаем работу с pandas\n* Научимся работать с Kaggle Notebooks\n* Поймем как делать предобработку различных данных\n* Научимся работать с пропущенными данными (Nan)\n* Познакомимся с различными видами кодирования признаков\n* Немного попробуем [Feature Engineering](https://ru.wikipedia.org/wiki/Конструирование_признаков) (генерировать новые признаки)\n* И совсем немного затронем ML\n* И многое другое...   \n\n\n\n### И самое важное, все это вы сможете сделать самостоятельно!\n\n*Этот Ноутбук являетсся Примером/Шаблоном к этому соревнованию (Baseline) и не служит готовым решением!*   \nВы можете использовать его как основу для построения своего решения.\n\n> что такое baseline решение, зачем оно нужно и почему предоставлять baseline к соревнованию стало важным стандартом на kaggle и других площадках.   \n**baseline** создается больше как шаблон, где можно посмотреть как происходит обращение с входящими данными и что нужно получить на выходе. При этом МЛ начинка может быть достаточно простой, просто для примера. Это помогает быстрее приступить к самому МЛ, а не тратить ценное время на чисто инженерные задачи. \nТакже baseline являеться хорошей опорной точкой по метрике. Если твое решение хуже baseline - ты явно делаешь что-то не то и стоит попробовать другой путь) \n\nВ контексте нашего соревнования baseline идет с небольшими примерами того, что можно делать с данными, и с инструкцией, что делать дальше, чтобы улучшить результат.  Вообще готовым решением это сложно назвать, так как используются всего 2 самых простых признака (а остальные исключаются)."},{"metadata":{},"cell_type":"markdown","source":"# import"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline\n\n# Загружаем специальный удобный инструмент для разделения датасета:\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\nimport re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# всегда фиксируйте RANDOM_SEED, чтобы ваши эксперименты были воспроизводимы!\nRANDOM_SEED = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# зафиксируем версию пакетов, чтобы эксперименты были воспроизводимы:\n!pip freeze > requirements.txt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"DATA_DIR = '/kaggle/input/sf-dst-restaurant-rating/'\ndf_train = pd.read_csv(DATA_DIR+'/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'kaggle_task.csv')\nsample_submission = pd.read_csv(DATA_DIR+'/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ВАЖНО! дря корректной обработки признаков объединяем трейн и тест в один датасет\ndf_train['sample'] = 1 # помечаем где у нас трейн\ndf_test['sample'] = 0 # помечаем где у нас тест\ndf_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Подробнее по признакам:\n* City: Город \n* Cuisine Style: Кухня\n* Ranking: Ранг ресторана относительно других ресторанов в этом городе\n* Price Range: Цены в ресторане в 3 категориях\n* Number of Reviews: Количество отзывов\n* Reviews: 2 последних отзыва и даты этих отзывов\n* URL_TA: страница ресторана на 'www.tripadvisor.com' \n* ID_TA: ID ресторана в TripAdvisor\n* Rating: Рейтинг ресторана"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Как видим, большинство признаков у нас требует очистки и предварительной обработки."},{"metadata":{},"cell_type":"markdown","source":"# Cleaning and Prepping Data\nОбычно данные содержат в себе кучу мусора, который необходимо почистить, для того чтобы привести их в приемлемый формат. Чистка данных — это необходимый этап решения почти любой реальной задачи.   \n![](https://analyticsindiamag.com/wp-content/uploads/2018/01/data-cleaning.png)"},{"metadata":{},"cell_type":"markdown","source":" # Restaurant_id"},{"metadata":{},"cell_type":"markdown","source":"Посмотрим на параметр \"Restaurant_id\". Данные представляют собой строковые значения, пропусков нет. Проверим, все ли значения уникальны."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Restaurant_id'].nunique(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Restaurant_id'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('Restaurant_id')['Rating'].agg('mean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Создадим новый признак, в который передадим Restaurant_id в числовом формате. Одновременно создадим список, в который будем сохранять признаки, которые не понадобятся на этапе ML.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_list = list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Restaurant_id_num'] = data['Restaurant_id'].apply(lambda x: int(x[3::]))\ndrop_list.append('Restaurant_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Restaurant_id_num', 'Rating']].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Как видно из корреляционного анализа, есть заметная обратная зависимость рейтинга от id ресторана."},{"metadata":{},"cell_type":"markdown","source":"# City"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['City'].nunique(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"В этом столбце также нет пропусков, и всего 31 уникальное значение. Эти данные можно будет использовать для создание дополнительных признаков. Для начала создадим dummy-переменные. Чтобы не потерять оригинальный столбец, который планируется использовать для генерации других признаков, dummy-переменные создаем на основании дублираованного столбца."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['City_code'] = data['City']\ndata = pd.get_dummies(data, columns=[ 'City_code',], dummy_na=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Среди городов есть столицы. Создадим столбец, указывающий на этот признак."},{"metadata":{"trusted":true},"cell_type":"code","source":"capitals = ['Athens', 'Berlin', 'Bratislava', 'Brussels', 'Budapest', 'Copenhagen', 'Dublin', 'Helsinki',\n            'Lisbon', 'Ljubljana', 'London', 'Luxembourg', 'Madrid', 'Oslo', 'Paris', 'Rome', 'Stockholm',\n            'Vienna', 'Warsaw','Zurich']    \ndata['is_capital'] = data['City'].apply(lambda x: 1 if x in capitals else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['is_capital'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Также встречаются города из одной страны. Создадим новый признак \"Country\" и сгенерируем на его основе dummy-переменные. Этот признак создадим только для тех стран, которые представлены несколькими странами, так как в противном случае этот признак будет дублирующим для определенных городов."},{"metadata":{"trusted":true},"cell_type":"code","source":"def country(row):\n    if row['City'] in ['Berlin', 'Hamburg', 'Munich']:\n        return 'Germany'\n    elif row['City'] in ['Geneva', 'Zurich']:\n        return 'Switzerland' \n    elif row['City'] in ['Krakow', 'Prague', 'Warsaw']:\n        return 'Poland'\n    elif row['City'] in ['Lisbon', 'Oporto']:\n        return 'Portugal'\n    elif row['City'] in ['Lyon', 'Paris']:\n        return 'France'\n    elif row['City'] in ['Milan', 'Rome']:\n        return 'Italy'\n    else:\n        return None\n    \ndata['Country'] = data.apply(country, axis=1)\ndata = pd.get_dummies(data, columns=['Country'], dummy_na=True)\ndata.drop(['Country_nan'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Также можно добавить признак с населением грода. Для этого загрузим дополнительные данные."},{"metadata":{"trusted":true},"cell_type":"code","source":"city_pop = pd.read_excel('../input/european-city-population-and-area/PopulatinandArea.xlsx')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"city_pop.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"city_pop.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def population(row):\n    return int(city_pop[city_pop['City'] == row['City']]['Population'])\n\ndata['Population'] = data.apply(population, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Хотя при создании этого столбца не возникло ошибок, убедимся на всякий случай, что население для каждого города было успешно найдено.\ndata['Population'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Rating', 'Population']].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Так же в датасете есть данные о площоди города. Добавим признак 'Population_density'."},{"metadata":{"trusted":true},"cell_type":"code","source":"city_pop.columns.to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pop_density(row):\n    pop = row['Population']\n    area = city_pop[city_pop['City'] == row['City']]['Area in km2 '].values[0]\n    return pop/area\n\ndata['Population_density'] = data.apply(pop_density, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Population_density']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Rating', 'Population', 'Population_density']].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Также, используя таблицу city_pop, удобно создать столбец с странами для каждого города."},{"metadata":{"trusted":true},"cell_type":"code","source":"def country(row):\n    return city_pop[city_pop['City'] == row['City']]['Country'].values[0]\n\ndata['Country'] = data.apply(country, axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_list.append('Country')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Используя другой датасет, сгенерируем новые признаки 'Happiness' и 'GDP_perCapita', исходя из стран. В результате предварительного анализа, выяснилось, что 'Beer_perCapita', 'Spirit_perCapita', 'Wine_perCapita' дублируют 'GDP_perCapita', поэтому их не испоьзовали."},{"metadata":{"trusted":true},"cell_type":"code","source":"Happiness_alcohol = pd.read_csv('../input/happiness-and-alcohol-consumption/HappinessAlcoholConsumption.csv')\nHappiness_alcohol.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Happiness_alcohol.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Happiness_alcohol['Country'].to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# В датасете не совпадали названия нескольких стран, а именно для Чехии и Нидерландов, также не было данных для Англии и Шотландии по отдельности. \n# Это пришлось учесть при написании функции.\ndef happiness(row):\n    if row['Country'] in ['England', 'Scotland']:\n        return Happiness_alcohol[Happiness_alcohol['Country'] == 'United Kingdom']['HappinessScore'].values[0]\n    if row['Country'] == 'Netherland':\n        return Happiness_alcohol[Happiness_alcohol['Country'] == 'Netherlands']['HappinessScore'].values[0]\n    if row['Country'] == 'Czech':\n        return Happiness_alcohol[Happiness_alcohol['Country'] == 'Czech Republic']['HappinessScore'].values[0]\n    return Happiness_alcohol[Happiness_alcohol['Country'] == row['Country']]['HappinessScore'].values[0]\n\ndata['Happiness'] = data.apply(happiness, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def GDP(row):\n    if row['Country'] in ['England', 'Scotland']:\n        return Happiness_alcohol[Happiness_alcohol['Country'] == 'United Kingdom']['GDP_PerCapita'].values[0]\n    if row['Country'] == 'Netherland':\n        return Happiness_alcohol[Happiness_alcohol['Country'] == 'Netherlands']['GDP_PerCapita'].values[0]\n    if row['Country'] == 'Czech':\n        return Happiness_alcohol[Happiness_alcohol['Country'] == 'Czech Republic']['GDP_PerCapita'].values[0]\n    return Happiness_alcohol[Happiness_alcohol['Country'] == row['Country']]['GDP_PerCapita'].values[0]\n\ndata['GDP_PerCapita'] = data.apply(GDP, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_list.append('City')\ndrop_list.append('City_code_nan')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cuisine Style"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Cuisine Style'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Обработаем данные из столбца \"Cuisine Style\".\nДанные представляют собой строковые значения, которые выглядят как список из нескольких видов кухни, а также содержат пустые значения.\nОбработаем признак с помощью метода One-Hot encoding, а также создадим список всех видов кухни, который будем использовать для кодировки. "},{"metadata":{},"cell_type":"markdown","source":"Кроме того, создадим еще признак о количестве видов кухни."},{"metadata":{"trusted":true},"cell_type":"code","source":"Cuisine_list = set()\n\nfor styles in data['Cuisine Style']:\n    if pd.isna(styles):\n        continue    \n    else:\n        styles = styles[1:-1].split(', ')\n        for style in styles:\n            Cuisine_list.add(style[1:-1])\n\n# Здесь невозможно использовать функцию get_dummies, так как необходимо выделять виды кухни из строкового значения            \ndef find_item(cell):\n    if item in str(cell):\n        return 1\n    else:\n        return 0\n\nfor style in Cuisine_list:\n    item = style\n    name = 'Cuisine_{}'.format(item)\n    data[name] = data['Cuisine Style'].apply(find_item)\n    \n\n# Tак как здесь не использовалать функция get_dummies, необходимо дополнительно прописать функцию, для создания dummy-переменной в случае значения None\ndef fill_cuisine_nan(row):\n    if pd.isna(row['Cuisine Style']):\n        return 1\n    else:\n        return 0\n    \ndata['Cuisine_nan'] = data.apply(fill_cuisine_nan, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Cuisine_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def variety(row):\n    if pd.isna(row['Cuisine Style']):\n        return 0    \n    return len(row['Cuisine Style'].split(', '))\n        \ndata['Cuisine_variety'] = data.apply(variety, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Сгенерируем признак, в который запишем инфомацию, подается ли в ресторане национальная кухня."},{"metadata":{"trusted":true},"cell_type":"code","source":"National_cuisine = {'France': 'French', 'England': 'British', 'Spain': 'Spanish', 'Italy': 'Italian', 'Germany': 'German', 'Portugal': 'Portuguese',\n                    'Czech': 'Czech', 'Poland': 'Polish', 'Austria': 'Austrian', 'Netherland': 'Dutch', 'Belgium': 'Belgian', 'Switzerland': 'Swiss', \n                    'Sweden': 'Swedish', 'Hungary': 'Hungarian', 'Ireland': 'Irish', 'Denmark': 'Danish', 'Greece': 'Greek', 'Scotland': 'Scottish',\n                    'Norway': 'Norwegian', 'Finland': 'Finnish', 'Slovakia': 'Slovak', 'Luxembourg': 'Luximbourgish', 'Slovenia': 'Slovenian'}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"National_cuisine = {'France': 'French', 'England': 'British', 'Spain': 'Spanish', 'Italy': 'Italian', 'Germany': 'German', 'Portugal': 'Portuguese',\n                    'Czech': 'Czech', 'Poland': 'Polish', 'Austria': 'Austrian', 'Netherland': 'Dutch', 'Belgium': 'Belgian', 'Switzerland': 'Swiss', \n                    'Sweden': 'Swedish', 'Hungary': 'Hungarian', 'Ireland': 'Irish', 'Denmark': 'Danish', 'Greece': 'Greek', 'Scotland': 'Scottish',\n                    'Norway': 'Norwegian', 'Finland': 'Finnish', 'Slovakia': 'Slovak', 'Luxembourg': 'Luximbourgish', 'Slovenia': 'Slovenian'}\n\ndef national_cuisine(row):\n    if pd.isna(row[\"Cuisine Style\"]):\n        return 0\n    for value, key in National_cuisine.items():\n        if value == row['Country']:\n            if key in row['Cuisine Style']:\n                return 1\n    return 0\ndata['National_cuisine'] = data.apply(national_cuisine, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['National_cuisine'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Rating', 'National_cuisine']].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_list.append('Cuisine Style')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ranking"},{"metadata":{},"cell_type":"markdown","source":"В этом столбце нет пропусков. Посмотрим как изменяется Ranking по городам, а также проверим, есть ли связь между Ranking и id ресторана."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Построим график для десяти городов с найбольшим количеством ресторанов\nfor x in (data['City'].value_counts())[0:10].index:\n    data['Ranking'][data['City'] == x].hist(bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Из-за того, что количество ресторанов в городах разное, масимальное значение Ranking для рестоанов будет разным в разных городах. Необходимо отнормировать этот признак, относитльно города."},{"metadata":{"trusted":true},"cell_type":"code","source":"ranking_mean = data.groupby('City')['Ranking'].agg('mean')\nrest_num = data['City'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def norm(row):\n    return (row['Ranking']-ranking_mean[row['City']])/rest_num[row['City']]\n\ndata['Ranking_norm'] = data.apply(norm, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Построим на график после нормирования\nfor x in (data['City'].value_counts())[0:10].index:\n    data['Ranking_norm'][data['City'] == x].hist(bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in (data['City'].value_counts())[0:10].index:\n    data['Restaurant_id_num'][data['City'] == x].hist(bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Распределение 'Restaurant_id' тожу чувствительно к количеству ресторанов в городе. Нормируем и это значение."},{"metadata":{"trusted":true},"cell_type":"code","source":"id_mean = data.groupby('City')['Restaurant_id_num'].agg('mean')\n\ndef id_norm(row):\n    return (row['Restaurant_id_num']-id_mean[row['City']])/rest_num[row['City']]\n\ndata['Restaurant_id_norm'] = data.apply(id_norm, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in (data['City'].value_counts())[0:10].index:\n    data['Restaurant_id_norm'][data['City'] == x].hist(bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Ranking', 'Ranking_norm', 'Restaurant_id_num', 'Restaurant_id_norm', 'Rating']].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Так, с помощью нормирования удалось усилить корреляцию значений Ranking и Restaurant_id с рейтингом."},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_list.append('Ranking')\ndrop_list.append('Restaurant_id_num')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Price Range"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Price Range'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Price Range'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"В данном столбце много пропусков (~35%). Трансформируем значения в числа для упрощения работы."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3:$$$$\n# 2:$$ - $$$\n# 1: $\n# 0: NaN\ndef fill_price_r(row):\n    if row['Price Range'] == '$$$$':\n        return 3\n    if row['Price Range'] == '$$ - $$$':\n        return 2\n    if row['Price Range'] == '$':\n        return 1\n    return None\n    \ndata['Price Range'] = data.apply(fill_price_r, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Price Range'].median()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Заполним пропуски медианным значением. Но перед этим сохраним информацию о пропусках в отдельном столбце."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['no_price_data'] = data['Price Range'].apply(lambda x: 1 if pd.isna(x) else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['no_price_data'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Price Range'].fillna(data['Price Range'].median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Price Range'].hist(bins=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Number of Reviews"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Number of Reviews'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Здесь также присутствуют пропуски. Прежде, чем их заменить, также сохраним информацию о них в отдельном столбце."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['no_data_Number_of_Reviews'] = pd.isna(data['Number of Reviews']).astype('uint8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['no_data_Number_of_Reviews'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Посмотрим, как распределен признак."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Number of Reviews'].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Видно, что в большинстве случаев количество отзывов не превышает 100. Посмотрим на гистограмму в этом диапазоне."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['Number of Reviews']<100]['Number of Reviews'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_num_rev(row):\n    if pd.isna(row['Number of Reviews']):\n        return data[data['City'] == row['City']]['Number of Reviews'].median()\n    return row['Number of Reviews']\n    \ndata['Number of Reviews_filled'] = data.apply(fill_num_rev, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Number of Reviews', 'Number of Reviews_filled', 'Population', 'Ranking_norm', 'Population_density']].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Number of Reviews', 'Number of Reviews_filled', 'Rating']].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('City')['Number of Reviews'].agg(['mean', 'median'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['Number of Reviews']<100]['Number of Reviews'].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Заполнить пропуски каким-то конкретным значением на мой взгляд не будет корректным. Сгенерируем новый признак, который характеризовал бы середину диапазона, в который попадает то или иное значение количества отзывов. Для этого каждое значение разделим на 50 и округлим до целого."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Number of Reviews_50'] = data['Number of Reviews'].apply(lambda x: None if pd.isna(x) else round(x/50, 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Number of Reviews_50'].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Проверим, сможем ли мы заполнять пропуски медианным значением в зависимости от города."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('City')['Number of Reviews_50'].agg(['mean', 'median'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_num_rev(row):\n    if pd.isna(row['Number of Reviews_50']):\n        return data[data['City'] == row['City']]['Number of Reviews_50'].min()\n    return row['Number of Reviews_50']\n\ndata['Number of Reviews_50_min'] = data.apply(fill_num_rev, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Number of Reviews_50_min'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['Number of Reviews_50_min'] < 5]['Number of Reviews_50_min'].hist(bins=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Проверим, не потеряли ли мы коррелецию в результате проведенных манипуляций."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Number of Reviews', 'Number of Reviews_50','Number of Reviews_50_min', 'Rating']].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_list.append('Number of Reviews')\ndrop_list.append('Number of Reviews_50')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reviews"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Reviews'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"На первый взглд здесь всего лишь 2 пропуска, но если просмотреть значения, то можно заметить довольно много значений вида '[[],[]]', что также можно рассматривать как пустые значения. В среднем структура значений представляеи собой список из двух вложенных списков: текст отзыва и дата отзыва. Проверим, сколько отзывов модет быть вложено."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Reviews_num'] = data['Reviews'].apply(lambda x: 0 if str(x) == '[[], []]' else len(str(x).split('], [')[0].split(\"', '\")))\ndata['Reviews_num'].fillna(0, inplace=True)\ndata['Reviews_num'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Далее извлечем даты в отдельные столбцы, а также посчитаем время между двумя отзывами и как давно был сделан последний отзыв."},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_date(string):\n    pattern = re.compile('\\d\\d\\/\\d\\d\\/\\d\\d\\d\\d')\n    return pattern.findall(string)\n\n\ndef date_1(row):\n    if pd.isna(row['Reviews']):\n        return None\n    date_list = find_date(row['Reviews'])\n    if len(date_list) == 0:\n        return None\n    else:\n        date_1 = datetime.strptime(date_list[0], '%m/%d/%Y')\n        if len(date_list) > 1:\n            date_2 = datetime.strptime(date_list[1], '%m/%d/%Y')\n            if date_1 > date_2:\n                return date_1\n            else:\n                return date_2\n        return date_1\n\n\ndef date_2(row):\n    if pd.isna(row['Reviews']):\n        return None\n    date_list = find_date(row['Reviews'])\n    if len(date_list) in [0, 1]:\n        return None\n    else:\n        date_1 = datetime.strptime(date_list[0], '%m/%d/%Y')\n        date_2 = datetime.strptime(date_list[1], '%m/%d/%Y')\n        if date_1 > date_2:\n            return date_2\n        else:\n            return date_1\n\n\ndata['last_date_1'] = data.apply(date_1, axis=1)\ndata['last_date_2'] = data.apply(date_2, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Новые столбец с количеством дней между отзывами\ndef days_btw(row):\n    if pd.isna(row['last_date_2']):\n        return None\n    return (row['last_date_1']-row['last_date_2']).days\n\n\ndata['days_btw_rev'] = data.apply(days_btw, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['days_btw_rev'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Тут получилось довольно много пропусков. Посмотрим на распределение признака."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['days_btw_rev'].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Здесь можно наблюдать аналогичную ситуацию, как в случае с количеством отзывов ('Number of Reviews'). Для заполнения пропусков поступим также."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['days_btw_rev']<400]['days_btw_rev'].hist(bins=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_days_btw(row):\n    if pd.isna(row['days_btw_rev']):\n        return data[data['City'] == row['City']]['days_btw_rev'].median()\n    return row['days_btw_rev']\ndata['days_btw_rev_filled'] = data.apply(fill_days_btw, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['days_btw_rev_filled'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Новый признак, характеризующий диапазон, в который попадает значение 'days_btw_rev'\ndata['days_btw_rev_100'] = data['days_btw_rev'].apply(lambda x: None if x is None else round(x/100, 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Проверяем, есть ли зависимость от города\ndata.groupby('City')['days_btw_rev_100'].agg(['mean', 'median'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Функция для заполнения пропусков\ndef fill_days_btw(row):\n    if pd.isna(row['days_btw_rev_100']):\n        return data[data['City'] == row['City']]['days_btw_rev_100'].median()\n    return row['days_btw_rev_100']\n\ndata['days_btw_rev_100'] = data.apply(fill_days_btw, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Rating', 'days_btw_rev_filled', 'days_btw_rev_100']].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['days_btw_rev_100'] < 5]['days_btw_rev_100'].hist(bins=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Новый признак, характеризующий, как давно был оставлен последний отзыв. В качестве референса возьмем максимальное значение из всех\ndef days_since(row):\n    if pd.isna(row['last_date_1']):\n        return None\n    else:\n        return (data['last_date_1'].max()-row['last_date_1']).days\n\n\ndata['days_since_last_rev'] = data.apply(days_since, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Пропуски\ndata['days_since_last_rev'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Посмотрим на распределение признака\ndata['days_since_last_rev'].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['days_since_last_rev']<500]['days_since_last_rev'].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_days_since(row):\n    if pd.isna(row['days_since_last_rev']):\n        return data[data['City'] == row['City']]['days_since_last_rev'].median()\n    return row['days_since_last_rev']\ndata['days_since_last_rev_filled'] = data.apply(fill_days_since, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Новый признак, характеризующий диапазон, в который попадает значение 'days_since_last_rev'\ndata['days_since_last_rev_100'] = data['days_since_last_rev'].apply(lambda x: None if x is None else round(x/100, 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# По гистограмме видно, что вид распределения сохраняется после преобразования\ndata[data['days_since_last_rev_100']<5]['days_since_last_rev_100'].hist(bins=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Функция для заполнения пропусков\ndef fill_days_since(row):\n    if pd.isna(row['days_since_last_rev_100']):\n        return data[data['City'] == row['City']]['days_since_last_rev_100'].median()\n    return row['days_since_last_rev_100']\n\ndata['days_since_last_rev_100'] = data.apply(fill_days_since, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['days_since_last_rev_100']<5]['days_since_last_rev_100'].hist(bins=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Rating', 'days_since_last_rev_filled', 'days_since_last_rev_100']].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Попробуем определить положительный или негативный характер отзыва по ключевым словам"},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_list = ['good', 'lovely', 'great', 'perfect', 'awesome', 'best', 'excellent', 'pleasent',\n                'outstanding', 'exceptional', 'fantastic', 'wonderful', 'fabulous', 'worth'\n                'enjoyable', 'nice', 'cosy', 'tasty', 'delicious', 'friendly', 'decent', 'gem',\n                'happy']\n\n\nnegative_list = ['bad', 'terrible', 'awful', 'poor', 'unacceptable', 'worst', 'gross',\n                 'regret', 'unpleasent', 'unsatisfactory', 'disgusting', 'unfriendly',\n                'rude', 'overpriced', 'slow', 'dissapoint', 'inadequate', 'weak', 'worse'\n                'unhappy', 'unfortunate']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def positive_rev(row):\n    a = str(row['Reviews']).lower().split('], [')[0].split(\"', '\")\n    for word in positive_list:\n        if len(a) == 2:\n            if ((word in a[0]) and not ('not' in a[0])) and (\n                (word in a[1]) and not ('not' in a[1])):\n                return 5\n            elif ((word in a[0]) and not ('not' in a[0])) or (\n                (word in a[1]) and not ('not' in a[1])):\n                return 4\n        elif len(a) == 1:\n            if ((word in a[0]) and not ('not' in a[0])):\n                return 4\n    for word in negative_list:\n        if len(a) == 2:\n            if ((word in a[0]) and not ('not' in a[0])) and (\n                (word in a[1]) and not ('not' in a[1])):\n                return 1\n            elif ((word in a[0]) and not ('not' in a[0])) or (\n                (word in a[1]) and not ('not' in a[1])):\n                return 2\n        elif len(a) == 1:\n            if ((word in a[0]) and not ('not' in a[0])):\n                return 2\n    return 3\n\ndata['Positive_rev'] = data.apply(positive_rev, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Positive_rev'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Rating', 'Positive_rev']].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_list.append('Reviews')\ndrop_list.append('last_date_1')\ndrop_list.append('last_date_2')\ndrop_list.append('days_btw_rev')\ndrop_list.append('days_since_last_rev')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ID_TA"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['ID_TA'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['ID_TA']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"В данном столбце пропусков нет. Можно заметить,что каждое значение выглядит как числовой код с буквой 'd' в начале. Создадим новый признак, куда передадим эти данные в числовом формате."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['ID_TA_num'] = data['ID_TA'].apply(lambda x: int(x[1::]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['ID_TA_num', 'Rating']].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"drop_list.append('ID_TA')"},{"metadata":{},"cell_type":"markdown","source":"# URL_TA"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['URL_TA'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[0,'URL_TA']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Значения в этом случае имеют несколько числовых последовательностей, начинающихся с 'g' и 'd'. Вторая последовательность дублирует значения в ID_TA. Поэтому выделим в отдельный признак только первую последовательность."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['URL_TA_num'] = data['URL_TA'].apply(lambda x: int(x.split('-')[1][1::]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['URL_TA_num', 'Rating']].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_list.append('URL_TA')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Посмотрим распределение целевой переменной 'Rating'"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Rating'].value_counts().plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='Rating', data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_list.append('Rating')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### И один из моих любимых - [корреляция признаков](https://ru.wikipedia.org/wiki/Корреляция)\nНа этом графике уже сейчас вы сможете заметить, как признаки связаны между собой и с целевой переменной."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Ranking', 'Restaurant_id_num']].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('City')['Rating'].agg(['min', 'max', 'mean', 'median'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Rating'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"city_rating_mean = data.groupby('City')['Rating'].agg('mean')\ncity_rating_mean['Berlin']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['City'].value_counts()['Berlin']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef city_rating_mean_norm(row):\n    return (row['Rating']-city_rating_mean[row['City']])/data['City'].value_counts()[row['City']]\n    \ndata['city_rating_mean_norm'] = data.apply(city_rating_mean_norm, axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Rating', 'city_rating_mean_norm']].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['city_rating_mean_norm']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"city_rating_mean = data.groupby('City')['Rating'].agg('mean')\ndef city_rev_num_mean_norm(row):\n    return (row['Number of Reviews_filled']-city_rating_mean[row['City']])/data['City'].value_counts()[row['City']]\n    \ndata['city_rev_num_mean_norm'] = data.apply(city_rev_num_mean_norm, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Rating', 'city_rev_num_mean_norm', 'Number of Reviews_filled']].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing\nТеперь, для удобства и воспроизводимости кода, завернем всю обработку в одну большую функцию."},{"metadata":{"trusted":true},"cell_type":"code","source":"# на всякий случай, заново подгружаем данные\ndf_train = pd.read_csv(DATA_DIR+'/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'/kaggle_task.csv')\ndf_train['sample'] = 1 # помечаем где у нас трейн\ndf_test['sample'] = 0 # помечаем где у нас тест\ndf_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Перебором была определена комбинация тех признаков, которая показывала наилучший результат. Следовательно генерация только этих признаков была включена в следующую функцию."},{"metadata":{"trusted":true},"cell_type":"code","source":"def preproc_data(df_input):\n    '''includes several functions to pre-process the predictor data.'''\n    \n    \n    df_output = df_input.copy()\n    \n    df_output['Number of Reviews_filled_0'] = df_output['Number of Reviews'].fillna(0)\n    \n    df_output['Restaurant_id_num'] = df_output['Restaurant_id'].apply(lambda x: int(x[3::]))\n    print('Restaurant_id')\n    df_output['City_code'] = df_output['City']\n    df_output = pd.get_dummies(df_output, columns=[ 'City_code',], dummy_na=True)\n    \n    capitals = ['Athens', 'Berlin', 'Bratislava', 'Brussels', 'Budapest', 'Copenhagen', 'Dublin', 'Helsinki',\n            'Lisbon', 'Ljubljana', 'London', 'Luxembourg', 'Madrid', 'Oslo', 'Paris', 'Rome', 'Stockholm',\n            'Vienna', 'Warsaw','Zurich']    \n    df_output['is_capital'] =  df_output['City'].apply(lambda x: 1 if x in capitals else 0)\n    \n    def country(row):\n        if row['City'] in ['Berlin', 'Hamburg', 'Munich']:\n            return 'Germany'\n        elif row['City'] in ['Geneva', 'Zurich']:\n            return 'Switzerland' \n        elif row['City'] in ['Krakow', 'Prague', 'Warsaw']:\n            return 'Poland'\n        elif row['City'] in ['Lisbon', 'Oporto']:\n            return 'Portugal'\n        elif row['City'] in ['Lyon', 'Paris']:\n            return 'France'\n        elif row['City'] in ['Milan', 'Rome']:\n            return 'Italy'\n        else:\n            return None\n\n    df_output['Country'] = df_output.apply(country, axis=1)\n    df_output = pd.get_dummies(df_output, columns=['Country'], dummy_na=True)\n    df_output.drop(['Country_nan'], axis=1, inplace=True)\n    \n    def country(row):\n        return city_pop[city_pop['City'] == row['City']]['Country'].values[0]\n\n    df_output['Country'] = df_output.apply(country, axis=1)\n    \n    Happiness_alcohol = pd.read_csv('../input/happiness-and-alcohol-consumption/HappinessAlcoholConsumption.csv')\n\n    def happiness(row):\n        if row['Country'] in ['England', 'Scotland']:\n            return Happiness_alcohol[Happiness_alcohol['Country'] == 'United Kingdom']['HappinessScore'].values[0]\n        if row['Country'] == 'Netherland':\n            return Happiness_alcohol[Happiness_alcohol['Country'] == 'Netherlands']['HappinessScore'].values[0]\n        if row['Country'] == 'Czech':\n            return Happiness_alcohol[Happiness_alcohol['Country'] == 'Czech Republic']['HappinessScore'].values[0]\n        return Happiness_alcohol[Happiness_alcohol['Country'] == row['Country']]['HappinessScore'].values[0]\n\n    df_output['Happiness'] = df_output.apply(happiness, axis=1)\n    \n    def GDP(row):\n        if row['Country'] in ['England', 'Scotland']:\n            return Happiness_alcohol[Happiness_alcohol['Country'] == 'United Kingdom']['GDP_PerCapita'].values[0]\n        if row['Country'] == 'Netherland':\n            return Happiness_alcohol[Happiness_alcohol['Country'] == 'Netherlands']['GDP_PerCapita'].values[0]\n        if row['Country'] == 'Czech':\n            return Happiness_alcohol[Happiness_alcohol['Country'] == 'Czech Republic']['GDP_PerCapita'].values[0]\n        return Happiness_alcohol[Happiness_alcohol['Country'] == row['Country']]['GDP_PerCapita'].values[0]\n\n    df_output['GDP_PerCapita'] = df_output.apply(GDP, axis=1)\n    \n    print('City')\n    \n    Cuisine_list = set()\n\n    for styles in df_output['Cuisine Style']:\n        if pd.isna(styles):\n            continue    \n        else:\n            styles = styles[1:-1].split(', ')\n            for style in styles:\n                Cuisine_list.add(style[1:-1])\n\n    def find_item(cell):\n        if item in str(cell):\n            return 1\n        else:\n            return 0\n\n    for style in Cuisine_list:\n        item = style\n        name = 'Cuisine_{}'.format(item)\n        df_output[name] =df_output['Cuisine Style'].apply(find_item)\n\n    def fill_cuisine_nan(row):\n        if pd.isna(row['Cuisine Style']):\n            return 1\n        else:\n            return 0\n    \n    df_output['Cuisine_nan'] = df_output.apply(fill_cuisine_nan, axis=1)\n    \n    def variety(row):\n        if pd.isna(row['Cuisine Style']):\n            return 0    \n        return len(row['Cuisine Style'].split(', '))\n\n    df_output['Cuisine_variety'] = df_output.apply(variety, axis=1)\n    \n    National_cuisine = {'France': 'French', 'England': 'British', 'Spain': 'Spanish', 'Italy': 'Italian', 'Germany': 'German', 'Portugal': 'Portuguese',\n                    'Czech': 'Czech', 'Poland': 'Polish', 'Austria': 'Austrian', 'Netherland': 'Dutch', 'Belgium': 'Belgian', 'Switzerland': 'Swiss', \n                    'Sweden': 'Swedish', 'Hungary': 'Hungarian', 'Ireland': 'Irish', 'Denmark': 'Danish', 'Greece': 'Greek', 'Scotland': 'Scottish',\n                    'Norway': 'Norwegian', 'Finland': 'Finnish', 'Slovakia': 'Slovak', 'Luxembourg': 'Luximbourgish', 'Slovenia': 'Slovenian'}\n\n    def national_cuisine(row):\n        if pd.isna(row[\"Cuisine Style\"]):\n            return 0\n        for value, key in National_cuisine.items():\n            if value == row['Country']:\n                if key in row['Cuisine Style']:\n                    return 1\n        return 0\n    \n    df_output['National_cuisine'] = df_output.apply(national_cuisine, axis=1)\n    \n    print('Cuisine')\n    \n    ranking_mean = df_output.groupby('City')['Ranking'].agg('mean')\n    rest_num = df_output['City'].value_counts()\n    \n    def norm(row):\n        return (row['Ranking']-ranking_mean[row['City']])/rest_num[row['City']]\n\n    df_output['Ranking_norm'] = df_output.apply(norm, axis=1)\n    \n    print('Ranking')\n    \n    df_output['no_price_data'] =  df_output['Price Range'].apply(lambda x: 1 if x is None else 0)\n    \n    def fill_price_r(row):\n        if row['Price Range'] == '$$$$':\n            return 3\n        if row['Price Range'] == '$$ - $$$':\n            return 2\n        if row['Price Range'] == '$':\n            return 1\n        return 2\n    \n    df_output['Price Range'] = df_output.apply(fill_price_r, axis=1)\n    \n    print('Price Range')\n    \n    df_output['no_data_Number_of_Reviews'] = pd.isna(df_output['Number of Reviews']).astype('uint8')\n    \n    city_rating_mean = df_output.groupby('City')['Rating'].agg('mean')\n    \n    def fill_num_rev(row):\n        if pd.isna(row['Number of Reviews']):\n            return df_output[df_output['City'] == row['City']]['Number of Reviews'].median()\n        return row['Number of Reviews']\n\n    df_output['Number of Reviews_filled'] = df_output.apply(fill_num_rev, axis=1)  \n    \n    print('Number of Reviews')\n    \n    df_output['Reviews_num'] = df_output['Reviews'].apply(lambda x: 0 if str(x) == '[[], []]' else len(str(x).split('], [')[0].split(\"', '\")))\n    df_output['Reviews_num'].fillna(0, inplace=True)\n    \n    print(1)\n    \n    def find_date(string):\n        pattern = re.compile('\\d\\d\\/\\d\\d\\/\\d\\d\\d\\d')\n        return pattern.findall(string)\n\n\n    def date_1(row):\n        if pd.isna(row['Reviews']):\n            return None\n        date_list = find_date(row['Reviews'])\n        if len(date_list) == 0:\n            return None\n        else:\n            date_1 = datetime.strptime(date_list[0], '%m/%d/%Y')\n            if len(date_list) > 1:\n                date_2 = datetime.strptime(date_list[1], '%m/%d/%Y')\n                if date_1 > date_2:\n                    return date_1\n                else:\n                    return date_2\n            return date_1\n\n\n    def date_2(row):\n        if pd.isna(row['Reviews']):\n            return None\n        date_list = find_date(row['Reviews'])\n        if len(date_list) in [0, 1]:\n            return None\n        else:\n            date_1 = datetime.strptime(date_list[0], '%m/%d/%Y')\n            date_2 = datetime.strptime(date_list[1], '%m/%d/%Y')\n            if date_1 > date_2:\n                return date_2\n            else:\n                return date_1\n\n\n    df_output['last_date_1'] = df_output.apply(date_1, axis=1)\n    df_output['last_date_2'] = df_output.apply(date_2, axis=1)\n    \n    print(2)\n    \n    def days_btw(row):\n        if pd.isna(row['last_date_2']):\n            return None\n        else:\n            return (row['last_date_1']-row['last_date_2']).days\n\n\n    df_output['days_btw_rev'] = df_output.apply(days_btw, axis=1)\n    \n    def fill_days_btw(row):\n        if pd.isna(row['days_btw_rev']):\n            return df_output[df_output['City'] == row['City']]['days_btw_rev'].median()\n        return row['days_btw_rev']\n    df_output['days_btw_rev_filled'] = df_output.apply(fill_days_btw, axis=1)\n    print(3)\n    \n\n    \n    def days_since(row):\n        if pd.isna(row['last_date_1']):\n            return None\n        else:\n            return (df_output['last_date_1'].max()-row['last_date_1']).days\n\n\n    df_output['days_since_last_rev'] = df_output.apply(days_since, axis=1)\n    \n    def fill_days_since(row):\n        if pd.isna(row['days_since_last_rev']):\n            return df_output[df_output['City'] == row['City']]['days_since_last_rev'].median()\n        return row['days_since_last_rev']\n    df_output['days_since_last_rev_filled'] = df_output.apply(fill_days_since, axis=1)\n    print(6)\n\n    \n    positive_list = ['good', 'lovely', 'great', 'perfect', 'awesome', 'best', 'excellent', 'pleasent',\n                'outstanding', 'exceptional', 'fantastic', 'wonderful', 'fabulous', 'worth'\n                'enjoyable', 'nice', 'cosy', 'tasty', 'delicious', 'friendly', 'decent', 'gem',\n                'happy']\n\n\n    negative_list = ['bad', 'terrible', 'awful', 'poor', 'unacceptable', 'worst', 'gross',\n                 'regret', 'unpleasent', 'unsatisfactory', 'disgusting', 'unfriendly',\n                'rude', 'overpriced', 'slow', 'dissapoint', 'inadequate', 'weak', 'worse'\n                'unhappy', 'unfortunate']\n    \n    def positive_rev(row):\n        a = str(row['Reviews']).lower().split('], [')[0].split(\"', '\")\n        for word in positive_list:\n            if len(a) == 2:\n                if ((word in a[0]) and not ('not' in a[0])) and (\n                    (word in a[1]) and not ('not' in a[1])):\n                    return 5\n                elif ((word in a[0]) and not ('not' in a[0])) or (\n                    (word in a[1]) and not ('not' in a[1])):\n                    return 4\n            elif len(a) == 1:\n                if ((word in a[0]) and not ('not' in a[0])):\n                    return 4\n        for word in negative_list:\n            if len(a) == 2:\n                if ((word in a[0]) and not ('not' in a[0])) and (\n                    (word in a[1]) and not ('not' in a[1])):\n                    return 1\n                elif ((word in a[0]) and not ('not' in a[0])) or (\n                    (word in a[1]) and not ('not' in a[1])):\n                    return 2\n            elif len(a) == 1:\n                if ((word in a[0]) and not ('not' in a[0])):\n                    return 2\n        return 3\n\n    df_output['Positive_rev'] =  df_output.apply(positive_rev, axis=1)\n    print('Reviews')\n        \n    df_output['ID_TA_num'] = df_output['ID_TA'].apply(lambda x: int(x[1::]))\n    \n    df_output['URL_TA_num'] = df_output['URL_TA'].apply(lambda x: int(x.split('-')[1][1::]))\n\n    return df_output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">По хорошему, можно было бы перевести эту большую функцию в класс и разбить на подфункции (согласно ООП). "},{"metadata":{},"cell_type":"markdown","source":"#### Запускаем и проверяем что получилось"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preproc = preproc_data(data)\ndf_preproc.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preproc.drop(['Restaurant_id',\n 'City',\n 'Cuisine Style',\n 'Number of Reviews',\n 'Reviews',\n 'URL_TA',\n 'ID_TA',\n 'Country',\n 'Number of Reviews_filled',\n 'Reviews_num',\n 'last_date_1',\n 'last_date_2',\n 'days_btw_rev',\n 'days_since_last_rev',\n 'Restaurant_id_num',  \n 'National_cuisine',\n 'no_data_Number_of_Reviews',], axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preproc.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Теперь выделим тестовую часть\ntrain_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)\ntest_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.Rating.values            # наш таргет\nX = train_data.drop(['Rating'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Перед тем как отправлять наши данные на обучение, разделим данные на еще один тест и трейн, для валидации. \nЭто поможет нам проверить, как хорошо наша модель работает, до отправки submissiona на kaggle.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Воспользуемся специальной функцие train_test_split для разбивки тестовых данных\n# выделим 20% данных на валидацию (параметр test_size)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# проверяем\ntest_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model \nСам ML"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Импортируем необходимые библиотеки:\nfrom sklearn.ensemble import RandomForestRegressor # инструмент для создания и обучения модели\nfrom sklearn import metrics # инструменты для оценки точности модели","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Создаём модель (НАСТРОЙКИ НЕ ТРОГАЕМ)\nmodel = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Обучаем модель на тестовом наборе данных\nmodel.fit(X_train, y_train)\n\n# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.\n# Предсказанные значения записываем в переменную y_pred\ny_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Сравниваем предсказанные значения (y_pred) с реальными (y_test), и смотрим насколько они в среднем отличаются\n# Метрика называется Mean Absolute Error (MAE) и показывает среднее отклонение предсказанных значений от фактических.\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# в RandomForestRegressor есть возможность вывести самые важные признаки для модели\nplt.rcParams['figure.figsize'] = (10,10)\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(15).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_temp = df_preproc.loc[data['sample'] == 1, list(feat_importances.nlargest(15).index[0:15])]\nplt.rcParams['figure.figsize'] = (12,6)\nax = sns.heatmap(df_temp.corr(), annot=True, fmt='.2g')\ni, k = ax.get_ylim()\nax.set_ylim(i+0.5, k-0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission\nЕсли все устраевает - готовим Submission на кагл"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = test_data.drop(['Rating'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_submission = model.predict(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['Rating'] = predict_submission\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}