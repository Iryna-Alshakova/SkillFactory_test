{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_brand = ['bmw', 'volkswagen', 'nissan', 'mercedes','toyota',\n",
    "         'audi', 'mitsubishi', 'skoda', 'volvo', 'honda',\n",
    "         'infiniti', 'lexus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for brand in car_brand:\n",
    "    car_list = {'car_url': [], 'prod_year': [], 'mileage': [], 'body': [], 'color': [],\n",
    "               'motor': [], 'transmission': [], 'gear': [], 'stir': [], 'condition':[],\n",
    "               'owners': [], 'PTS': [], 'customs': [], 'price': []}\n",
    "    auto_list_global = [] # здесь соберем список списков (эл-т - список параметров конкретного автомобиля)\n",
    "\n",
    "    pages_list = list(range(1, 11)) # количество страниц для парсинга \n",
    "    with tqdm(total=len(pages_list)) as pbar:\n",
    "        err=0\n",
    "        k=0\n",
    "        for i in pages_list:\n",
    "            url = f'https://auto.ru/cars/{brand}/used/?output_type=list&page={i}'\n",
    "            response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            response.encoding = 'utf-8'\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            page = soup.find_all('a', class_ = 'Link ListingItemTitle-module__link')\n",
    "            links_list=[]\n",
    "            for j in range(len(page)):\n",
    "                links_list.append(str(page[j]).split('\"')[3])\n",
    "            for link in links_list:\n",
    "                k+=1\n",
    "                car_soup = BeautifulSoup(requests.get(link, headers={'User-Agent': 'Mozila/5.0'}).text, 'html.parser')\n",
    "                try:\n",
    "                    car_soup.find_all('span', class_ = 'CardInfoRow__cell')[1].text.encode('l1').decode().replace('\\xa0', '')\n",
    "                    car_soup.find_all('span', class_ = 'OfferPriceCaption__price')[0]\n",
    "                except:\n",
    "                    err+=1\n",
    "                    print(k, link, err)\n",
    "                    print('Error')\n",
    "                    continue #Тут отсекаются страницы с машинами, которые уже проданы\n",
    "                car_list['car_url'].append(link)\n",
    "                l=1\n",
    "                for key in list(car_list.keys())[1:-1]:\n",
    "                    car_list[key].append(car_soup.find_all('span', class_ = 'CardInfoRow__cell')[l].text.encode('l1').decode().replace('\\xa0', ''))\n",
    "                    l+=2\n",
    "                car_list['price'].append(car_soup.find_all('span', class_ = 'OfferPriceCaption__price')[0].text.encode('l1').decode().replace('\\xa0', ''))\n",
    "            pbar.update(1)\n",
    "    a = pd.DataFrame.from_dict(car_list, orient='columns')\n",
    "    a.to_csv(f'{brand}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a = pd.read_csv('bmv_2.csv')\\nb = pd.read_csv('volkswagen_2.csv')\\nc= pd.read_csv('nissan_2.csv')\\nd=pd.read_csv('mercedes_2.csv')\\ne=pd.read_csv('toyota_2.csv')\\nf=pd.read_csv('audi_2.csv')\\ng=pd.read_csv('mitsubishi_2.csv')\\nh=pd.read_csv('skoda_2.csv')\\ni=pd.read_csv('volvo_2.csv')\\nj=pd.read_csv('honda_2.csv')\\nk = pd.read_csv('infiniti_2.csv')\\nl = pd.read_csv('lexus_2.csv')\\nm = pd.read_csv('honda_3.csv')\\nn = pd.read_csv('honda_4.csv')\\na =pd.concat([a, b, c, d, e, f, g, h, i, j, k, l, m, n])\\na.info()\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(f'bmw.csv')\n",
    "for brand in car_brand[1::]:\n",
    "    brand = pd.read_csv(f'{brand}.csv')\n",
    "    data = pd.concat([data, brand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 28584 entries, 0 to 968\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   car_url       28584 non-null  object\n",
      " 1   prod_year     28584 non-null  int64 \n",
      " 2   mileage       28584 non-null  object\n",
      " 3   body          28584 non-null  object\n",
      " 4   color         28584 non-null  object\n",
      " 5   motor         28584 non-null  object\n",
      " 6   transmission  28584 non-null  object\n",
      " 7   gear          28584 non-null  object\n",
      " 8   stir          28584 non-null  object\n",
      " 9   condition     28584 non-null  object\n",
      " 10  owners        28584 non-null  object\n",
      " 11  PTS           28584 non-null  object\n",
      " 12  customs       28584 non-null  object\n",
      " 13  price         28584 non-null  object\n",
      "dtypes: int64(1), object(13)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data = data.drop_duplicates(data.drop_duplicates(subset=['prod_year',\n",
    " 'mileage',\n",
    " 'body',\n",
    " 'color',\n",
    " 'motor',\n",
    " 'transmission',\n",
    " 'gear',\n",
    " 'stir',\n",
    " 'condition',\n",
    " 'owners',\n",
    " 'PTS',\n",
    " 'customs',\n",
    " 'price'], inplace=True))\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b831101288435f84c32cc88d177a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for brand in ['mercedes']:\n",
    "    car_list = {'car_url': [], 'prod_year': [], 'mileage': [], 'body': [], 'color': [],\n",
    "               'motor': [], 'transmission': [], 'gear': [], 'stir': [], 'condition':[],\n",
    "               'owners': [], 'PTS': [], 'customs': [], 'price': []}\n",
    "    auto_list_global = [] # здесь соберем список списков (эл-т - список параметров конкретного автомобиля)\n",
    "\n",
    "    pages_list = list(range(1, 2)) # количество страниц для парсинга \n",
    "    with tqdm(total=len(pages_list)) as pbar:\n",
    "        err=0\n",
    "        k=0\n",
    "        \n",
    "        url = f'https://auto.ru/cars/{brand}/used/?body_type_group=LIMOUSINE'\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        response.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        page = soup.find_all('a', class_ = 'Link ListingItemTitle-module__link')\n",
    "        links_list=[]\n",
    "        for j in range(len(page)):\n",
    "            links_list.append(str(page[j]).split('\"')[3])\n",
    "        for link in links_list:\n",
    "            k+=1\n",
    "            car_soup = BeautifulSoup(requests.get(link, headers={'User-Agent': 'Mozila/5.0'}).text, 'html.parser')\n",
    "            try:\n",
    "                car_soup.find_all('span', class_ = 'CardInfoRow__cell')[1].text.encode('l1').decode().replace('\\xa0', '')\n",
    "                car_soup.find_all('span', class_ = 'OfferPriceCaption__price')[0]\n",
    "            except:\n",
    "                err+=1\n",
    "                print(k, link, err)\n",
    "                print('Error')\n",
    "                continue #Тут отсекаются страницы с машинами, которые уже проданы\n",
    "            car_list['car_url'].append(link)\n",
    "            l=1\n",
    "            for key in list(car_list.keys())[1:-1]:\n",
    "                car_list[key].append(car_soup.find_all('span', class_ = 'CardInfoRow__cell')[l].text.encode('l1').decode().replace('\\xa0', ''))\n",
    "                l+=2\n",
    "            car_list['price'].append(car_soup.find_all('span', class_ = 'OfferPriceCaption__price')[0].text.encode('l1').decode().replace('\\xa0', ''))\n",
    "        pbar.update(1)\n",
    "    a = pd.DataFrame.from_dict(car_list, orient='columns')\n",
    "    a.to_csv(f'{brand}_limousine.csv',index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
